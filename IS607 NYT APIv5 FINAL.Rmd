---
title: "IS607 Final Project"
author: "Andrew Goldberg"
date: "December 13, 2015"
output: html_document
---

```{r}
library(jsonlite)
library(RCurl)
library(stringr)
library(XML)
library(tm)
library(stm)
library(SnowballC)
library(koRpus)
library(textreg)
library(ggplot2)
library(dplyr)
library(tidyr)
library(wordcloud)
options(nytimesid = "55c21eb10d7a056ad11b584434ad1554:10:73343003")
dir.create("goldberg")
setwd("C:/Users/Andrew/Desktop/Cuny/Data Acquisition/Final Project/")

nytcolumnists <- c('brooks', "dowd", "bruni", "collins", "douthat", "friedman", "kristof", "krugman")
fullnytcolumnists <- c("DAVID+BROOKS", "MAUREEN+DOWD", "FRANK+BRUNI", "GAIL+COLLINS", "ROSS+DOUTHAT", "THOMAS+L+FRIEDMAN", "NICHOLAS+KRISTOF", "PAUL+KRUGMAN")

#Create vector (may go back to fix as list) of link names
nytcollink <- vector(length = 0)
for(i in nytcolumnists){nytcollink <- append(nytcollink, str_c(i, "link"))}

#Create list of corpus names
nytcolcorp <- as.list(paste0(nytcolumnists,"corp"))
names(nytcolcorp) <- nytcolumnists

#Another list of names
for(i in nytcolumnists){nytcolcorp <- append(nytcolcorp, str_c(i, "corp"))}
rm(nytcolcorp)

#Collect 20 article links per author
for (i in 1:length(nytcolumnists)){
  n <- vector(length = 0)
  for(ii in 1:2){
    tmp <- paste0('http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=section_name:("Opinion")&fq=byline:("',fullnytcolumnists[i],'")&begin_date=20140101&end_date=20151212&sort=newest&page=',ii)
    tmp <- getForm(tmp, `api-key` = getOption("nytimesid"), .opts=list())
    tmp <- fromJSON(tmp)
    tmp <- tmp$response$docs$web_url
    n <- append(n, tmp)
  }
  assign(nytcollink[i], n)
}
head(brookslink)
getwd()
setwd(paste0("C:/Users/Andrew/Desktop/Cuny/Data Acquisition/Final Project/",nytcolumnists[i]))


setwd(paste0("C:/Users/Andrew/Desktop/Cuny/Data Acquisition/Final Project/",nytcolumnists[i]))
#Download html content for all articles and store in columnist-named folder
for (i in 1:length(nytcolumnists)){
  setwd("C:/Users/Andrew/Desktop/Cuny/Data Acquisition/Final Project")
  dir.create(nytcolumnists[i])
  setwd(paste0("C:/Users/Andrew/Desktop/Cuny/Data Acquisition/Final Project/",nytcolumnists[i]))
  for (ii in seq_along(get(nytcollink[i]))){
    fname <- str_c(ii, ".html")
    try(download.file((get(nytcollink[i]))[ii], fname))
  }
}

#Reads html files into a corpus
for (i in 1:length(nytcolumnists)){
  setwd(paste0("C:/Users/Andrew/Desktop/Cuny/Data Acquisition/Final Project",nytcolumnists[i]))
  tmp <- readLines("1.html")
  tmp <- str_c(tmp, collapse = "")
  tmp <- htmlParse(tmp)
  tmp <- (xpathApply(tmp, "//div/p[@itemprop='articleBody']", xmlValue))
  tmp <- str_c(tmp, collapse = "")
  release <- (iconv(tmp, "latin1", "ASCII", sub=""))
  nytcolcorp[[i]] <- Corpus(VectorSource(release))
  meta(nytcolcorp[[i]], "byline") <- nytcolumnists[i]
  
  for(ii in 2:length(list.files(pattern=".html"))){
    tmp <- readLines(str_c(ii,".html"))
    tmp <- str_c(tmp, collapse = "")
    tmp <- htmlParse(tmp)
    tmp <- xpathApply(tmp, "//div/p[@itemprop='articleBody']", xmlValue)
    tmp <- str_c(tmp, collapse = "")
    release <- (iconv(tmp, "latin1", "ASCII", sub=""))
    
    if (length(release) !=0){
      tmp_corpus <- Corpus(VectorSource(release))
      nytcolcorp[[i]] <- c(nytcolcorp[[i]], tmp_corpus, recursive = FALSE)
      meta(nytcolcorp[[i]], "byline") <- nytcolumnists[i]
    }
  }
}
table(byline)

#Combine corpus objects
nytCombCorp <- c(nytcolcorp[[1]], nytcolcorp[[2]], nytcolcorp[[3]], nytcolcorp[[4]], nytcolcorp[[5]], nytcolcorp[[6]], nytcolcorp[[7]], nytcolcorp[[8]], recursive=T)
table(meta(nytCombCorp, tag = "byline"))

#Clean and simplify data
nytCombCorp <- tm_map(nytCombCorp, removeNumbers)
nytCombCorp <- tm_map(nytCombCorp, removePunctuation)
nytCombCorp <- tm_map(nytCombCorp, stemDocument)
nytCombCorp <- tm_map(nytCombCorp, removeWords, stopwords('english'))

#Create randomized corpus
nytCombCorpR <- sample(nytCombCorp)
nytCombCorpR

#Build document-term matrix. I worried about simplifying with spam. 
nytDTM <- DocumentTermMatrix(nytCombCorpR)

#Collect meta labels
bylines <- unlist(meta(nytCombCorpR, "byline")[,1])
table(bylines)
```

Explore data
```{r}
#Some light statistical analysis to describe the data, highest percent associations
findAssocs(nytDTM, "clinton", .5)
findAssocs(nytDTM, "obama", .6)

#Word cloud
wordcloud(nytCombCorpR, scale=c(5,0.5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))
```

Train and run models
```{r}
#Prepare container
N <- length(nytDTM)
container <- create_container(
  nytDTM,
  labels = bylines,
  trainSize = 1:40,
  testSize = 41:N,
  virgin = FALSE
)

slotNames(container)

#Training models
svm_model <- train_model(container, "SVM")
rf_model <- train_model(container, "RF")
maxent_model <- train_model(container, "MAXENT")

#Classifying data
svm_out <- classify_model(container, svm_model)
rf_out <- classify_model(container, rf_model)
maxent_out <- classify_model(container, maxent_model)

head(svm_out)
head(rf_out)
head(maxent_out)
```

Visually, maxentropy appears to have the strongest prediction rates
```{r}
ggplot(svm_out, aes(x = SVM_PROB)) + geom_histogram(binwidth = .01)
svh <- ggplot(svm_out, aes(x = SVM_PROB, fill = SVM_LABEL)) 
svh + geom_histogram(binwidth = .05) + facet_wrap(~SVM_LABEL)

ggplot(maxent_out, aes(x = MAXENTROPY_PROB)) + geom_histogram(binwidth = .01)
maxh <- ggplot(maxent_out, aes(x = MAXENTROPY_PROB, fill = MAXENTROPY_LABEL)) 
maxh + geom_histogram(binwidth = .05) + facet_wrap(~MAXENTROPY_LABEL)

ggplot(rf_out, aes(x = FORESTS_PROB)) + geom_histogram(binwidth = .01)
frh <- ggplot(rf_out, aes(x = FORESTS_PROB, fill = FORESTS_LABEL)) 
frh + geom_histogram(binwidth = .05) + facet_wrap(~FORESTS_LABEL)
```

Inspecting the statistics, maxentropy results have nearly double the mean of the next best, random forests, with well over 3 standard deviations in difference. 
```{r}
summary(svm_out$SVM_PROB)
sd(svm_out$SVM_PROB)

summary(maxent_out$MAXENTROPY_PROB)
sd(maxent_out$MAXENTROPY_PROB)

summary(rf_out$FORESTS_PROB)
sd(rf_out$FORESTS_PROB)
```

```{r}
max <- data.frame(
  maxlab = maxent_out$MAXENTROPY_LABEL,
  maxscore = maxent_out$MAXENTROPY_PROB)

maxT <- max %>%
  group_by(maxlab) %>%
  summarize(average=mean(maxscore))

maxT
#max_ent appeared not to have enough sample to properly train and classify Dowd

svm <- data.frame(
  svmlab = svm_out$SVM_LABEL,
  svmscore = svm_out$SVM_PROB)

svmT <- svm %>%
  group_by(svmlab) %>%
  summarize(average=mean(svmscore))

svmT
#support vector machines performed strongest among bruni and krugman

rf <- data.frame(
  rflab = rf_out$FORESTS_LABEL,
  rfscore = rf_out$FORESTS_PROB)

rfT <- rf %>%
  group_by(rflab) %>%
  summarize(average=mean(rfscore))

rfT
#random forests performed best among krugman, collins, bruni, and dowd -- the most liberal columnists
```


Interested in running some other types of analysis in KoRpus on political speeches!
```{r}
#Downloaded txt files, not scraped or api??
originalspeeches <- "http://www.cs.cmu.edu/~ark/CLIP/data/raw-speeches.tar.gz"
download.file(originalspeeches, destfile="ospeeches.tar.gz")
untar("ospeeches.tar.gz")

edwardsprimary <- Corpus(DirSource("edwards-speeches/primary2008"), readerControl = list(language="lat"))
bidenprimary <- Corpus(DirSource("biden-speeches/primary2008"), readerControl = list(language="lat"))
clintonprimary <- Corpus(DirSource("clinton_h-speeches/primary2008"), readerControl = list(language="lat"))

#koRpus likes vectors
edwardsvect <- convert.tm.to.character(edwardsprimary)
bidenvect <- convert.tm.to.character(bidenprimary)
clintonvect <- convert.tm.to.character(clintonprimary)

#Collapse vectors
edwardsC <- str_c(edwardsvect, collapse = "")
bidenC <- str_c(bidenvect, collapse = "")
clintonC <- str_c(clintonvect, collapse = "")

#Tokenize vectors
taggededwards <- tokenize(edwardsC, format="obj", lang="en")
taggedbiden <- tokenize(bidenC, format="obj", lang="en")
taggedclinton <- tokenize(clintonC, format="obj", lang="en")

head(taggedText(taggedbiden))
str(describe(taggedbiden))
describe(taggedbiden)[["lttr.distrib"]]
lex.div(taggedbrooks)

edwards.hypth.txt.en <- hyphen(taggededwards)
biden.hypth.txt.en <- hyphen(taggedbiden)
clinton.hypth.txt.en <- hyphen(taggedclinton)

edwards.readbl.txt <- readability(taggededwards, hyp=edwards.hypth.txt.en, index="all")
biden.readbl.txt <- readability(taggedbiden, hyp=biden.hypth.txt.en, index="all")
clinton.readbl.txt <- readability(taggedclinton, hyp=clinton.hypth.txt.en, index="all")

summary(edwards.readbl.txt)
summary(biden.readbl.txt)
summary(clinton.readbl.txt)
```

This is what it ended up as because biden left early (.csv!)
```{r}
originalspeeches <- "http://www.cs.cmu.edu/~ark/CLIP/data/raw-speeches.tar.gz"
prim08res <- read.csv("http://www.electoral-vote.com/evp2008/Data/primaries.csv")
primdat <- data.frame(prim08res$State, prim08res$Clinton, prim08res$Obama, prim08res$Edwards)

head(primdat)
primRes <- primdat %>%
  gather(State, cand, 2:4) %>%
  group_by(State) %>%
  summarize(average=mean(cand, na.rm= TRUE))
#Averaged over the full race, Clinton held more aggregate support!
```{r}